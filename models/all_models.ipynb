{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.telenor.rs/media/TelenorSrbija/media/Telenor_horizontalni.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "<h1><center>TELENOR DIGITAL: SMARTE BYGG</center></h1>\n",
    "<h2><center> Summer project 2019 </center><h2>\n",
    "<h4><center> By Maria Hilmo Jensen, May Helen Storvik and Odd Eirik Igland </center><h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example markdown:\n",
    "\n",
    "# This is a level 1 heading\n",
    "## This is a level 2 heading\n",
    "This is some plain text that forms a paragraph.\n",
    "Add emphasis via **bold** and __bold__, or *italic* and _italic_.\n",
    "Paragraphs must be separated by an empty line.\n",
    "* Sometimes we want to include lists.\n",
    "* Which can be indented.\n",
    "1. Lists can also be numbered.\n",
    "2. For ordered lists.\n",
    "[It is possible to include hyperlinks](https://www.example.com)\n",
    "Inline code uses single backticks: `foo()`, and code blocks use triple backticks:\n",
    "```\n",
    "bar()\n",
    "```\n",
    "Or can be indented by 4 spaces:\n",
    "foo()\n",
    "And finally, adding images is easy: ![Alt text](https://www.example.com/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "Introduce the project and the desired outcome. \n",
    "\n",
    "Models we are using:\n",
    "* Linear regression\n",
    "* Simple time series model with Naive Bayes\n",
    "* Facebook Prophet for time series analysis\n",
    "* Feed Forward Neural Network\n",
    "* Catboost Decision Tree\n",
    "* LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the necessary packages and files are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# All the created prediction models\n",
    "from models.all_models import *\n",
    "from models.linear_regression.linear_regression.linear_regression import *\n",
    "from models.prophet.prophet_model import *\n",
    "from models.simple_time_series.simple_time_series import *\n",
    "from models.feed_forward.feed_forward import *\n",
    "from models.catboost_model.catboost_model import *\n",
    "from models.linear_regressionlstm.lstm import *\n",
    "#from analysis.parking_and_canteen import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Analysis\n",
    "The team were provided with raw canteen transaction data and parking data from Telenor. This part is about understanding the provided data and collecting more data from external APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking and canteen data\n",
    "parking_and_canteen.py: \n",
    "* Combinding the raw data from parking and canteen transactions.\n",
    "* Finding the correlation between parking and canteen (graph from Odd please)\n",
    "* Removing outliers\n",
    "* Fill in missing canteen data based on parking\n",
    "\n",
    "### Coorelation\n",
    "Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation_parking_canteen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display data\n",
    "Plots the canteen data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canteen_plot = dt_df_test.filter(['date', 'Canteen'])\n",
    "#plt.figure(figsize=(16,8))\n",
    "#plt.plot(canteen_df)\n",
    "#plt.title('Number of people at Telenor Oct 2016 - Feb 2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather\n",
    "weather.py:\n",
    "* Historic weather data are collected from the [Frost API](https://frost.met.no/index.html) provided by Meterologisk Institutt.\n",
    "* Using the weather station at Blindern (no. SN18700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays and vacation\n",
    "holiday.py:\n",
    "* Historic holiday data are collected from [WebAPI](https://webapi.no/api/v1/holidays/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination\n",
    "combined_dataset.py:\n",
    "* Merges the combined canteen/parking data with weather data and holidays/vacation/inneklemt\n",
    "* Stores the created dataframe into a .csv file in the data folder: dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/dataset.csv', index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preprocessing\n",
    "After the raw data files are combined into dataset.csv, the data needs to be preprocessed further before they are used by the models. All preprocessing methods are found in the preprocessing folder and are summarized in two methods in preprocessing.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including historic canteen data \n",
    "The team decided to add historic canteen data as columns to the data file because of the strong correlation between these numbers. This includes the number of canteen transactions made one week ago and the previous day.\n",
    "\n",
    "The following plots shows the relation between these numbers for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including weekday and time of the year\n",
    "We were also interested in finding the effect of the day of the week and time of year. Two colums were therefore added, one displaying the weekday and one variable with distance from start of the year (as a number between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the temperature\n",
    "The original data contained both maximum and minimum temperature. It was decided to combine these to one column by taking the average of the two values. We made the assumption that temperatures in spesicic intervals increases the probability that people stay home from work. Therefore, we categorized the average temperatures into two groups; Temperatures where you are more likely to skip work (stay_home_temp) and temperatures were you most likely go to work (preferred_work_temp). We assumed that very low temperatures (less than -10 degrees), temperatures around 0 degrees (from -2 to +2 degrees) and temperatures above +20 degrees are more likely to affect if people come to work, and these interval were therefore chosen to be our stay_home_temp. All other temperatures are preferred_working_temp. \n",
    "\n",
    "-10 <= x <= -2 || +2 <= x <= 20 (preferred_work_temp),\n",
    "x < -10 || -2 < x < +2 || x > 20 (stay_home_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting data format\n",
    "After the previously mentioned preprocessing steps were taken, it was necessary to create two different input data formats. The reason for this is that the machine learning models consider weighted inputs and the categories for each column are therefore used as new columns. All the other models can use the previously created format (now named decision tree dataframe).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading the data from dataset.csv and storing this as a DataFrame, we can then use the save_dataframes method in preprocessing.py. This method includes all the preprocessing steps and stores two new data files named decision_tree_df.csv and ml_df.csv in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data files\n",
    "After the new files are created they can be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df, ml_df = load_datafiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top five rows of the decision tree and machine learning data sets are shown respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Throughout this summer the team has worked on several different prediction models, both statistical and machine learning models. All the models, explaination and visualizations are presented below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, y_pred, x_test, y_test = linear(dataset) \n",
    "plot_linear(x, y, x_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple time series with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_pred = simple_time_series(dt_df, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Prophet\n",
    "Prophet is designed for analyzing time series with daily observations that display patterns on different time scales. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. It also has advanced capabilities for modeling the effects of holidays on a time-series.\n",
    "\n",
    "This model only consider the canteen data and date, as well as Norwegian holidays (provided by Prophet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet(dt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the models\n",
    "All the different models are compared based on how they perform on predicting the last given number days of our dataset, as well as 8 days into the future (from today).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test dataframe for model comparison\n",
    "All the models will be tested against the last given number of days of the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 8\n",
    "\n",
    "real_canteen, dt_df_test = create_dataframe_for_comparison(dt_df, test_size)\n",
    "_, ml_df_test = create_dataframe_for_comparison(ml_df, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = create_predictions(dt_df, dt_df_test, ml_df_test)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the prediction from the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_test_figures(real_canteen, merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
