{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.telenor.rs/media/TelenorSrbija/media/Telenor_horizontalni.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "<h1><center>TELENOR DIGITAL: SMARTE BYGG</center></h1>\n",
    "<h2><center> Summer project 2019 </center><h2>\n",
    "<h4><center> By Maria Hilmo Jensen, May Helen Storvik and Odd Eirik Igland </center><h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The main purpose of this project is to predict how many people that will come to work at Telenor Fornebu, for up to approximatly one week into the future. The number of transactions made in the canteens, as well as parking information for the time period October 2016 to February 2019, were provided. It was assumed that the number of people at work equals the number that eats in the canteens. The data set did not contain information about the canteen transactions for all the dates between the start date and end date. Based on the correlation between the parking data and the canteen transactions we were able to deduce how many people were at Telenor when canteen data were missing. \n",
    "\n",
    "The team also decided to consider outside parameters, such as temperature, precipitation, vacations, holidays, \"inneklemte dager\", day of the week and time of year. It was also assumed that no one is at work in the weekends, so the number of people working were set to zero. \n",
    "\n",
    "Several different prediction models were created to compare performance and determine suitability for the given data set. The models created were:\n",
    "* Linear regression\n",
    "* Simple time series model with Naive Bayes\n",
    "* Facebook Prophet for time series analysis\n",
    "* Feed Forward Neural Network\n",
    "* Catboost Decision Tree\n",
    "* LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_markdown"
    ]
   },
   "source": [
    "## Imports\n",
    "All the necessary packages and files are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.abspath(''), os.path.pardir)))\n",
    "\n",
    "from all_models import *\n",
    "from main import main\n",
    "from constants import DAYS_TO_TEST\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "To be able to use all the following methods it is necessary to create all the relevant .csv files. By running the main.py file, they will be added to the data folder. This method includes all the data analysis and preprocessing steps.\n",
    "\n",
    "To be able to run this method, it is necessary to have a folder called \"data\" containing \"kdr_transactions\" and \"parking_data\" folders with Excel data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "The team were provided with raw canteen transaction data and parking data from Telenor. This part of the report is about understanding the provided data and collecting more data from external APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking and canteen data\n",
    "The team created one main python file for processing the parking and canteen transaction data. This is the parking_and_canteen.py file, and it contains code for the following: \n",
    "* Combinding the raw data from parking and canteen transactions\n",
    "* Removing outliers (data points that differs significantly from other observations)\n",
    "* Finding the correlation/dependency between parking and canteen data\n",
    "* Fill in missing canteen data based on parking\n",
    "\n",
    "### Correlation\n",
    "In statistics, dependence is any statistical relationship between two random variables or bivariate data. Correlation is defined as any statistical dependence, though it is commonly referred to as the degree to which a pair of variables are linearly related. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.\n",
    "\n",
    "To determine if we could supply the missing canteen data using the provided parking data, it was necessary to first calculate the correlation between these two variables. A high correlation indicates a strong dependency and the range is from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the pairwise relationship between parking and canteen data. The diagonally descending axis show the univariate distribution of the data for the variable in that column. A univariate distribution is a probability distribution of only one random variable.\n",
    "\n",
    "As we can see from the calculated correlation and the graphs above, there is a strong relationship between the two variables. We can therefore exploit their dependency to supply the missing canteen data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display data\n",
    "The following plot displays the complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_canteen_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the plot above the number of people on weekdays normally fluctuates between 1500 and 2000 people. It is also possible to spot the trend were the number of people significantly decline during for example summer vacation, easter and christmas.\n",
    "\n",
    "A trend for the whole period is a decending number of people. Before the main holidays it is also a decending trend, which is bigger than the ascending trend after the holidays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather\n",
    "Historical weather data was collected from the [Frost API](https://frost.met.no/index.html) provided by Meterologisk Institutt. The team decided to use the weather station at Blindern (no. SN18700) because this was the closest weather station containing all necessary data. \n",
    "\n",
    "It was decided to only use the precipitation and temparature, because these variables were considered to have most impact on whether people show up for work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays and vacation\n",
    "Holidays, vacations and \"inneklemte dager\" will affect the use of the office spaces. They are defined as:\n",
    "* Holidays: All days registered as official Norwegian holidays\n",
    "* Vacation: Summer, winter and autumn vacation given by the Oslo and Akershus municipality\n",
    "* \"Inneklemte dager\": One or two working days that falls between two days off from work (either holiday or weekend)\n",
    "\n",
    "Historical holiday data were collected using [WebAPI](https://webapi.no/api/v1/holidays/). An algorithm for finding \"inneklemte dager\" was created. The code can be found in holiday.py.\n",
    "\n",
    "As seen in the figure above, summer vacation has a bigger impact on the amount of working people than autumn and winter vacation. Holidays affect even more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination\n",
    "combined_dataset.py:\n",
    "* Merges the combined canteen/parking data with weather data and holidays/vacation/inneklemt\n",
    "* Stores the created dataframe into a .csv file in the data folder: dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/dataset.csv', index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "After the raw data files are combined into dataset.csv, the data needs to be preprocessed further before they are used by the models. All preprocessing methods are found in the preprocessing folder and are summarized in one method in preprocessing.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including historic canteen data \n",
    "The team discovered that the number of canteen transactions made one week ago and the previous day had a strong correlation to the current date.\n",
    "\n",
    "The following plots shows the relation between these numbers for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation_historic_canteen_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the correlation with previous day is quite low. By looking at the plot is's possible to see that many dots are parallel to the axes, which could indicate a quick change in canteen visitors. Changes like that effects the correlation with previous day, removing the data from the weekends removes some of these situations.\n",
    "\n",
    "The plot below is without data from weekends, as expected this gives a higher correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation_historic_canteen_no_weekend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including weekday and time of the year\n",
    "It is interesting to find the effect of the day of the week and time of year. Two colums were therefore added, one displaying the weekday and one variable with distance from start of the year (as a number between 0 and 1).\n",
    "\n",
    "The plot below shows mean values for each weekday, it's clear to see that they have an effect. Since the data set is around two years it's possible that the models will see this weakly trend. However the column for time of the year will most likely not have an effect on the final prediction, since the models will not be able to see a yearly trend over two years. That column is therefore added for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_workers_per_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the temperature\n",
    "The original data contained both maximum and minimum temperature. It was decided to combine these to one column by finding the average of the two values. The assumption that temperatures in specific intervals increases the probability that people stay home from work was made. Therefore, the average temperatures was categorized into two groups: \n",
    "* Temperatures where you are more likely to skip work (stay_home_temp)\n",
    "* Temperatures were you most likely go to work (preferred_work_temp)\n",
    "\n",
    "It was assumed that very low temperatures (less than or equal to -10 degrees), temperatures around 0 degrees (from -2 to +2 degrees) and temperatures above +20 degrees are more likely to affect if people come to work, and these intervals were therefore chosen to be our stay_home_temp. All other temperatures are preferred_work_temp. \n",
    "\n",
    "Preferred work temp: $x \\in (-10, -2], \\enspace x \\in (2, 20]$\n",
    "\n",
    "Stay home temp: $x \\in (-\\infty, -10], \\enspace x \\in (-2, 2], \\enspace x \\in (20, \\infty)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether or not the weather affects the number of people eating in the canteens, the correlation was calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation_weather_canteen(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the obtained correlation and plot above, it appears to be a very small dependence between weather and number of people that come to work. Based on the lack of statistical dependence, the team decided to remove weather data from the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resulting data format\n",
    "After the previously mentioned preprocessing steps were taken, it was necessary to create two different input data formats: one containing categorical values and one without. The reason for this is that the machine learning models consider weighted inputs and the input needs to be normalized between 0 and 1. In order to do this all columns must have integer values and can therefore not be categorical.\n",
    "\n",
    "After following the instructions in [Setup](#setup) all necessary files have been added to the data folder. This stores among others two new data files named decision_tree_df.csv and ml_df.csv in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data files\n",
    "After the new files are created they can be loaded.\n",
    "The top five rows of the decision tree and machine learning data sets are shown respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df, ml_df = load_datafiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Throughout this summer the team has worked on several different prediction models, both statistical and machine learning models. All the models, explaination and visualizations are presented below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Linear regression is a linear approach to modeling the relationship between two variables. The linear() function uses the inputs date and historic canteen values in order to predict future values. \n",
    "\n",
    "A model using linear regression will try to draw a straight line that will minimize the residual sum of squares between the observed responses in the dataset. (Source: https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, y_pred, x_test, y_test = linear(dataset) \n",
    "plot_linear(x, y, x_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our canteen data fluctuates, a straight line will not manage to create a good prediction of future values as can be seen in the graph above. The mean absolute error of the linear regression model is usually around 700-800 people. \n",
    "\n",
    "The model performs slightly better if weekends are removed from the dataset, but the result is still not sufficient compared to the other models and is therefore disregarded for this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple time series with Naive Bayes for forecasting\n",
    "This model takes advantage of one approach that is commonly known as binning (discretization or reducing the scale level of a random variable from numerical to categorical). An advantage of this technique is the reduction of noise - however, this comes at the cost of losing quite an amount of information.\n",
    "\n",
    "This model only considers the canteen data as a function of time (date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_time_series(dt_df, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph displays the real canteen values and the predicted canteen values for the data set. As can be seen from the plot the prediction discovers some of the trends in the data set, but fails to detect the decreasing trend over time. The mean absolute error for this model is 181 people, which is quite high but still a major improvement compared to the linear regression. \n",
    "\n",
    "Since this is a time series model, it is made for predicting the data set's consecutive values. Therefore, when this model are going to predict the future, it has to predict every day between the last day in the data set and today before it can come up with a prediction for future dates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Prophet\n",
    "[Prophet](https://facebook.github.io/prophet/) is designed for analyzing time series with daily observations that display patterns on different time scales. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. It also has advanced capabilities for modeling the effects of holidays on a time-series.\n",
    "\n",
    "Also this model only considers the canteen data and date, as well as Norwegian holidays (provided by Prophet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet(dt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot displays the prediction made by Prophet. The black dots represents the real values, the dark blue lines is the predictions and the light blue lines are a 95 % confidence interval for the predictions. As can be seen, the model fits some of the data points and are able to detect trends from holidays. The MAE for this model is 70 people, which is a great improvement to the simple time series model. The model performs surprisingly well for only considering people over time and holidays. \n",
    "\n",
    "A major drawback to this model is that it has to predict the values directy following the data set, in the same way the simple time series model works. This implies that in order to perform well the model require recent canteen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward model is made using Tensorflow with Keras. It's a simple neural network with one hidden layer. This model is trained with the machine learning dataset.\n",
    "\n",
    "20% of the dataset is used as a test set. From the plots below it's possible to see that the training data is decreasing by the number of epochs, and stays close to the validation error. This indicates that the model is not overfitted.\n",
    "\n",
    "The plot on the right side shows the quality of the prediction. The red line shows true values, so dots close to the line are good predictions. This model is most accurate when many people are working, probabiliy because that's when the model have most of its data. The model that is currently stored performs well with an MAE score of 66 people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_and_prediction_ml(\n",
    "    load_model_sav(\"feed_forward_history\"),\n",
    "    load_model_sav(\"feed_forward_epoch\"),\n",
    "    load_model_sav(\"feed_forward_test_set_prediction\")\n",
    ")\n",
    "\n",
    "print_mae(ml_df, \"feed_forward_test_set_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost is a Python library for making decision trees, for both regression and classification problems. A decision tree split the data set by one attribute at the time. Attributes that splits most of the data set are chosen first. The data you want to predict goes through the tree, and end up with the predicted value in the leaf node. Since this is a regression problem it's the number of people in the canteen.\n",
    "\n",
    "By looking at the train and validation error both flats out around 40-50 in Mean Absolute Error (MAE). The graph on the right shows quite good results. Compared to feed forward this model has dots closer to the red line, which indicates that this is a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_and_prediction_df(\n",
    "    load_model_sav(\"catboost_evaluation_result\"), \n",
    "    load_model_sav(\"catboost_test_set_prediction\")\n",
    ")\n",
    "print_mae(ml_df, \"catboost_test_set_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model was made using a recurrent neural network with LSTM (Long Short Term Memory). The model was made using Tensorflow, which provides a high level API called Keras for modeling and training of neural networks. \n",
    "\n",
    "The model input includes all relevant external factors that might affect the amount of people that go to work a given day in the near future. All input is scaled to be in the range of 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_and_prediction_ml(\n",
    "    load_model_sav(\"lstm_history\"),\n",
    "    load_model_sav(\"lstm_epoch\"),\n",
    "    load_model_sav(\"lstm_test_set_prediction\")\n",
    ")\n",
    "\n",
    "print_mae(ml_df, \"lstm_test_set_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM performs slightly worse than the stored Catboost and Feed Forward model with a MAE score of 67 people. The score of the different neural networks (LSTM and Feed Forward) will vary if trained from scratch. This is due to the weights getting different initialization values each time the model is trained. \n",
    "\n",
    "The stored neural network models have the lowest possible MAE score received after being run 60 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the models\n",
    "All the different models are compared based on how they perform on predicting the last given number days of our dataset, as well as 8 days into the future (from today).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data from data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create test dataframe for model comparison\n",
    "All the models will be tested against the last given number of days of the full data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_canteen, dt_df_test = create_dataframe_for_comparison(dt_df, DAYS_TO_TEST)\n",
    "_, ml_df_test = create_dataframe_for_comparison(ml_df, DAYS_TO_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree test dataframe looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning dataframe looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the predictions\n",
    "The following table displays the predictions from all the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = create_predictions(dt_df, ml_df, dt_df_test, ml_df_test, False, real_canteen)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_test_predictions(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Catboost and LSTM are the models predicting most correctly. The simple time series model with Naive Bayes performs worst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the future\n",
    "We start by getting the data sets for the next days. These sets are created with the same format as the test data sets from the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_next, ml_next = load_next_days()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe for decision trees looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows all the predictions for the next 8 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_merged = create_predictions(dt_df, ml_df, dt_next, ml_next)\n",
    "future_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_test_predictions(future_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the figure above, the prediction results vary greatly among the different models and due to the lack of data it is not possible to draw any conclusion on which model performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on the achieved results, you can see that Catboost performs best, followed by the feed forward model. This is based on the amount of data, with a bigger data set other results could occur. Since LSTM also uses its previous prediction as input it might achieve better results on a bigger data set. It's hard to draw any conclusions on which models are preferred and therefore none of the models are dismissed at this point. Right now, all of the models (except linear and STS) performs with a MAE between 46-70 which is relatively close to each other. \n",
    "\n",
    "\n",
    "At Telenor the weather parameters have a poor correlation to how many people show up at work any given day. These parameters are included in the data set as it might affect other companies differently. Also, some of the other input parameters probably makes no differece at the moment, but several years of data could give models that predict yearly trends.\n",
    "\n",
    "Use this prediction to:\n",
    "* Avoid food waste, as a good prediction model will make it possible for the canteen(s) to scale the amount of food according to how many people are at work. In those cases where a company has several canteens it is necessary to find the distribuiton of people based on for example the canteen menu. \n",
    "\n",
    "\n",
    "* Save energy by closing floors in quiet periods. If a company has free seating it can close down certain floors on days when less people are expected at work. This will help save the environment and save money for the company, as lights, air-conditioning/heating can be turned off for the closed floor.  \n",
    "\n",
    "\n",
    "* Strenghten the utilization algorithm from Telenor. The utilization algorithm is an algorithm that Telenor wants to improve for fiding the utilization of the different rooms in a smart building (IoT equipped building). An example of this would be a meeting room, how often is it used and when is it usually busy. By adding our prediction model it would also be possible to get a better prediction on when it will be busy in the future (next 8 days).   \n",
    "\n",
    "Next steps:\n",
    "* Update daily with new data to provide a stronger prediction model. Continuous supply of new data will improve the prediction made by the models. Now, the last entry in the data set is 24th of February 2019, which means that the models needs to predict future values without any recent data. The neural networks will perform significantly worse as they are lacking the data from a day and a week ago. The time series models (Prophet and STS) are made for predicting the data set's consecutive values. Therefore, when these models are going to predict the future, they have to predict every day between the last day in the data set and today before they can come up with a prediction for future dates.\n",
    "\n",
    "\n",
    "* If accessible it would be preferred to use entry gates data (or similar) instead of canteen data, as it will be more accurate. Here at Telenor it has been assumed that everyone eats in the canteens, as this was the only available data at this time. The provided canteen data might therefore deviate from actual number of people. Entry gate data will consistently be more accurate, regardless of company. The amount of people who eats in the canteens will vary to a greater extent. \n",
    "\n",
    "\n",
    "* Make models more general, so they can be used by other buildings and companies. The first part of the preprocessing needs to be individualized to accomodate for different data set formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
